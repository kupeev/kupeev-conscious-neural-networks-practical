<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c11{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:87pt;border-top-color:#000000;border-bottom-style:solid}.c8{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:170.2pt;border-top-color:#000000;border-bottom-style:solid}.c2{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:153pt;border-top-color:#000000;border-bottom-style:solid}.c4{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:74.2pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:127.5pt;border-top-color:#000000;border-bottom-style:solid}.c25{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:italic}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c26{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Courier New";font-style:normal}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c10{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c17{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11.5pt;font-family:"Arial";font-style:normal}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.3571428571428572;text-align:left}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c3{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c18{margin-left:12pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c23{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c19{color:inherit;text-decoration:inherit}.c6{background-color:#ffffff}.c24{height:188pt}.c9{height:11pt}.c22{height:76.7pt}.c16{color:#434343}.c14{height:125.4pt}.c20{height:78.8pt}.c0{height:0pt}.c21{height:178.5pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c6 c23 doc-content"><p class="c15 c9"><span class="c1"></span></p><p class="c15"><span>Description of the code for &quot;Interpretable Semiotics Networks Representing Awareness&quot; paper by David Kupeev and Eyal Nitcany, 2023.<br><br>The python code is located in &lt;code&gt; folder of &nbsp;</span><span class="c3"><a class="c19" href="https://www.google.com/url?q=https://github.com/kupeev/conscious-neural-networks-practical&amp;sa=D&amp;source=editors&amp;ust=1699292913103526&amp;usg=AOvVaw1Y10Qj6oBRZ48lFI26Cf-t">https://github.com/kupeev/conscious-neural-networks-practical</a></span><span class="c1">&nbsp;repository.</span></p><p class="c15"><span>References are given wrt the root directory of the repo.</span></p><p class="c15 c9"><span class="c1"></span></p><p class="c9 c15"><span class="c1"></span></p><a id="t.886de6c569c4508afc949044c2606e3188a080e3"></a><a id="t.0"></a><table class="c18"><tr class="c22"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">functionality</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">scripts</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">input</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">output folder[s],</span></p><p class="c5"><span class="c1">or</span></p><p class="c5"><span class="c1">output file[s]</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">reference </span></p><p class="c5"><span class="c1">to the CONN </span></p><p class="c5"><span class="c1">flowchart notation</span></p><p class="c5"><span class="c1">(aux_info/overall2.png)</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">bipartite orbits</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">orbits.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5 c9"><span class="c1"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;out&gt;: snapshots</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">&lt;models&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5 c9"><span class="c1"></span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">restricted MNIST</span></p><p class="c5"><span class="c1">datasets</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c6 c9 c13"><span class="c26"></span></p><p class="c5 c9"><span class="c1"></span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">created following </span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">Michael Nielsen,</span></p><p class="c13 c6"><span class="c25">RMNIST Repository,</span></p><p class="c13 c6"><span class="c1">2017,</span></p><p class="c13 c6"><span class="c3"><a class="c19" href="https://www.google.com/url?q=https://github.com/mnielsen/rmnist&amp;sa=D&amp;source=editors&amp;ust=1699292913113991&amp;usg=AOvVaw1SnH9P9TSpYIMGMHgN6qzF">https://github.com/mnielsen/rmnist</a></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5 c9"><span class="c1"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;data/rmnist.aug0&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op0</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">training the models of the </span></p><p class="c5"><span class="c1">autoencoder ( Appendix G)</span></p><p class="c5"><span class="c1">part1:</span></p><p class="c5"><span class="c1">epoch 0:100000</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">train_autoenc_1.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">restricted MNIST</span></p><p class="c5"><span class="c1">datasets</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;models&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op1</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">training the models of the </span></p><p class="c5"><span class="c1">autoencoder Appendix G</span></p><p class="c5"><span class="c1">part2:</span></p><p class="c5"><span class="c1">epoch 100000:110000</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">train_autoenc_2.py</span></p><p class="c5 c9"><span class="c1"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">restricted MNIST</span></p><p class="c5"><span class="c1">datasets,</span></p><p class="c5"><span class="c1">models of the</span></p><p class="c5"><span class="c1">autoencoders ( part1)</span></p><p class="c5 c9"><span class="c1"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;models&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op1</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">creation of perceptualized input for the stochastic &nbsp;classifier</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">make_ATR_ATE_data_stoch.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">restricted MNIST</span></p><p class="c5"><span class="c1">datasets</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">models of the</span></p><p class="c5"><span class="c1">autoencoders</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;data&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op2op3</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">creation of &nbsp;perceptualized input for the vanilla classifier</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">make_ATR_ATE_data_vanilla.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">restricted MNIST</span></p><p class="c5"><span class="c1">datasets</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">models of the</span></p><p class="c5"><span class="c1">autoencoders</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;data&gt;</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op2op3</span></p></td></tr><tr class="c0"><td class="c11" colspan="1" rowspan="1"><p class="c5"><span class="c1">creation of the &nbsp;test data (TE) for the CONN and the benchmark classifiers</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">make_TE_for_benchmark_classifier.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5 c9"><span class="c1"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">&lt;data&gt;</span></p><p class="c5"><span class="c1">(saved in data\test_data)</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c5"><span class="c1">op5</span></p></td></tr><tr class="c21"><td class="c11" colspan="1" rowspan="3"><p class="c5"><span class="c1">comparative plotting of accuracies for the stochastic CONN classifier, the vanilla CONN</span></p><p class="c5"><span class="c1">classifier, and the benchmark classifier for different numbers of training epochs</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c6 c12">aside2.py</span></p><p class="c5 c9"><span class="c1"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">the ATR and ATE data for the stochastic classifier.</span></p><p class="c5"><span class="c1">the TE data for the benchmark classifier</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">OR </span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">the ATR and ATE data for the vanilla classifier.</span></p><p class="c5"><span class="c1">the TE data for the benchmark classifier</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1 c6">res_dict.npy</span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5 c9"><span class="c1 c6"></span></p><p class="c5"><span class="c1 c6">res_dict.npy</span></p></td><td class="c7" colspan="1" rowspan="3"><p class="c5"><span class="c1">op4,op5</span></p></td></tr><tr class="c14"><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c16">make_plots2.py</span></p><p class="c5 c9"><span class="c17 c6 c16"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c10">res_dict.npy</span></p><p class="c5"><span class="c16">for the stochastic </span><span>classifier</span></p><p class="c5 c9"><span class="c10"></span></p><p class="c5"><span class="c10">OR</span></p><p class="c5 c9"><span class="c10"></span></p><p class="c5"><span class="c10">res_dict.npy</span></p><p class="c5"><span class="c16">for the vanilla </span><span>classifier</span></p><p class="c5 c9"><span class="c10"></span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span>png files in </span><span class="c10">docs/out_make_plots2</span></p><p class="c5 c9"><span class="c10"></span></p><p class="c5 c9"><span class="c10"></span></p></td></tr><tr class="c0"><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c12 c6">make_plots2_3graphs.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">res_dict.npy</span></p><p class="c5"><span class="c1">files</span></p><p class="c5"><span class="c1">for the stock</span></p><p class="c5"><span class="c1">and the</span></p><p class="c5"><span class="c1">vanilla</span></p><p class="c5"><span class="c1">classifiers</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">png files in docs/out_make_plots2</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5"><span class="c1">(saved in docs\our_make_plots2.both_conns.reinit2)</span></p></td></tr><tr class="c24"><td class="c11" colspan="1" rowspan="2"><p class="c5"><span class="c1">plotting of accuracy values for the stochastic CONN classier (in blue), and the benchmark</span></p><p class="c5"><span class="c1">classier (in red), over series of 100 random initializations before training</span></p></td><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">aside2_100runs_std.py</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">ATR and ATE data for the stochastic classifier;</span></p><p class="c5"><span class="c1">TE data for the benchmark classifier </span></p><p class="c5"><span class="c1">(see aux_info/overall2.png)</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c6">res_dict.npy</span></p></td><td class="c7" colspan="1" rowspan="2"><p class="c5"><span class="c1">op4,op5</span></p></td></tr><tr class="c20"><td class="c4" colspan="1" rowspan="1"><p class="c5"><span class="c1">make_plots2_100runs_std.py</span></p><p class="c5 c9"><span class="c1"></span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c5"><span class="c1">res_dict.npy</span></p></td><td class="c2" colspan="1" rowspan="1"><p class="c5"><span class="c1">png files in docs\out_make_plots2</span></p><p class="c5 c9"><span class="c1"></span></p><p class="c5 c9"><span class="c1"></span></p></td></tr></table><p class="c15 c9"><span class="c1"></span></p><p class="c15 c9"><span class="c1"></span></p><p class="c15 c9"><span class="c1"></span></p></body></html>